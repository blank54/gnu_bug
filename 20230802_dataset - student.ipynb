{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a0bbb1",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10d6c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from urllib import request, parse\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "__file__ = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7534a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "FDIR_DATA = os.path.join(__file__, 'data')\n",
    "\n",
    "FDIR_URL_LIST = os.path.join(FDIR_DATA, 'news/hyundai_motors/url_list/')\n",
    "make_dir_soft(FDIR_URL_LIST)\n",
    "\n",
    "FDIR_ARTICLE = os.path.join(FDIR_DATA, 'news/hyundai_motors/article/')\n",
    "make_dir_soft(FDIR_ARTICLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b0fbe",
   "metadata": {},
   "source": [
    "# Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "765f44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_soft(fpath):\n",
    "    dirpath = os.path.dirname(fpath)\n",
    "    if os.path.isdir(dirpath):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "    return None\n",
    "\n",
    "def remove_date_sep(date):\n",
    "    return data.replace('.', '')\n",
    "\n",
    "def calculate_date_list(date_start, duration):\n",
    "    date_start_by_datetime = datetime.strptime(date_start, '%Y.%m.%d')\n",
    "\n",
    "    date_list = []\n",
    "    for days_after in range(duration):\n",
    "        date_end_by_datetime = date_start_by_datetime + timedelta(days=days_after)\n",
    "        date_end = datetime.strftime(date_end_by_datetime, '%Y.%m.%d')\n",
    "\n",
    "        date_list.append(date_end)\n",
    "        \n",
    "    return date_list\n",
    "\n",
    "def request_for_soup(url, headers):\n",
    "    req = request.Request(url=url, headers=headers)\n",
    "    html = request.urlopen(req).read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def is_list_page_empty(soup):\n",
    "    if soup.find('div', class_='not_found02'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_sleep_time_random():\n",
    "    random_number = np.random.normal(1,0.1)\n",
    "    if random_number > 0:\n",
    "        pass\n",
    "    elif random_number < 0:\n",
    "        random_number = random_number * (-1)\n",
    "    else:\n",
    "        random_number = 1\n",
    "    \n",
    "    return random_number\n",
    "\n",
    "def parse_article(soup):\n",
    "    title = soup.find('h2').get_text()\n",
    "    date, _, _ = soup.find('span', class_='media_end_head_info_datestamp_time').get_text().split()\n",
    "    content = soup.find('div', id='dic_area').get_text()\n",
    "    \n",
    "    return title, date, content\n",
    "\n",
    "def import_url_dict(fdir):\n",
    "    url_dict = {}\n",
    "    for fname in os.listdir(fdir):\n",
    "        date, _ = fname.split('.')\n",
    "        \n",
    "        fpath = os.path.join(fdir, fname)\n",
    "        with open(fpath, 'r', encoding='utf-8') as f:\n",
    "            url_dict[date] = json.load(f)\n",
    "            \n",
    "    return url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837f2d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query parsed: %ED%98%84%EB%8C%80%EC%9E%90%EB%8F%99%EC%B0%A8\n",
      "Date list: 2023.07.01\n",
      "2023.07.02\n",
      "2023.07.03\n",
      "2023.07.04\n",
      "2023.07.05\n",
      "2023.07.06\n",
      "2023.07.07\n",
      "2023.07.08\n",
      "2023.07.09\n",
      "2023.07.10\n",
      "2023.07.11\n",
      "2023.07.12\n",
      "2023.07.13\n",
      "2023.07.14\n",
      "2023.07.15\n",
      "2023.07.16\n",
      "2023.07.17\n",
      "2023.07.18\n",
      "2023.07.19\n",
      "2023.07.20\n",
      "2023.07.21\n",
      "2023.07.22\n",
      "2023.07.23\n",
      "2023.07.24\n",
      "2023.07.25\n",
      "2023.07.26\n",
      "2023.07.27\n",
      "2023.07.28\n",
      "2023.07.29\n",
      "2023.07.30\n"
     ]
    }
   ],
   "source": [
    "url_base = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query={}&sort=1&photo=0&field=0&pd=3&ds={}&de={}&start={}'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "\n",
    "QUERY = '현대자동차'\n",
    "QUERY_PARSED = parse.quote(QUERY)\n",
    "\n",
    "DATE_START = '2023.07.01'\n",
    "DURATION = 30\n",
    "DATE_LIST = calculate_date_list(DATE_START, DURATION)\n",
    "\n",
    "print('Query parsed: {}'.format(QUERY_PARSED))\n",
    "print('Date list: {}'.format('\\n'.join(DATE_LIST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c1b5ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Date: 2023.07.01\n",
      "URL list (current day): 21\n",
      "URL list (total)      : 21\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.02\n",
      "URL list (current day): 62\n",
      "URL list (total)      : 83\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.03\n",
      "URL list (current day): 193\n",
      "URL list (total)      : 276\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.04\n",
      "URL list (current day): 153\n",
      "URL list (total)      : 429\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.05\n",
      "URL list (current day): 223\n",
      "URL list (total)      : 652\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.06\n",
      "URL list (current day): 126\n",
      "URL list (total)      : 778\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.07\n",
      "URL list (current day): 47\n",
      "URL list (total)      : 825\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.08\n",
      "URL list (current day): 23\n",
      "URL list (total)      : 848\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.09\n",
      "URL list (current day): 71\n",
      "URL list (total)      : 919\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.10\n",
      "URL list (current day): 149\n",
      "URL list (total)      : 1068\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.11\n",
      "URL list (current day): 118\n",
      "URL list (total)      : 1186\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.12\n",
      "URL list (current day): 129\n",
      "URL list (total)      : 1315\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.13\n",
      "URL list (current day): 218\n",
      "URL list (total)      : 1533\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.14\n",
      "URL list (current day): 96\n",
      "URL list (total)      : 1629\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.15\n",
      "URL list (current day): 8\n",
      "URL list (total)      : 1637\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.16\n",
      "URL list (current day): 50\n",
      "URL list (total)      : 1687\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.17\n",
      "URL list (current day): 54\n",
      "URL list (total)      : 1741\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.18\n",
      "URL list (current day): 136\n",
      "URL list (total)      : 1877\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.19\n",
      "URL list (current day): 99\n",
      "URL list (total)      : 1976\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.20\n",
      "URL list (current day): 212\n",
      "URL list (total)      : 2188\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.21\n",
      "URL list (current day): 59\n",
      "URL list (total)      : 2247\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.22\n",
      "URL list (current day): 27\n",
      "URL list (total)      : 2274\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.23\n",
      "URL list (current day): 61\n",
      "URL list (total)      : 2335\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.24\n",
      "URL list (current day): 111\n",
      "URL list (total)      : 2446\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.25\n",
      "URL list (current day): 139\n",
      "URL list (total)      : 2585\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.26\n",
      "URL list (current day): 355\n",
      "URL list (total)      : 2940\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.27\n",
      "URL list (current day): 141\n",
      "URL list (total)      : 3081\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.28\n",
      "URL list (current day): 108\n",
      "URL list (total)      : 3189\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.29\n",
      "URL list (current day): 17\n",
      "URL list (total)      : 3206\n",
      "------------------------------------------------------------\n",
      "Date: 2023.07.30\n",
      "URL list (current day): 9\n",
      "URL list (total)      : 3215\n",
      "============================================================\n",
      "Done: 3215\n"
     ]
    }
   ],
   "source": [
    "URL_COUNT = 0\n",
    "\n",
    "for date in DATE_LIST:\n",
    "    print('--'*30)\n",
    "    print('Date: {}'.format(date))\n",
    "    \n",
    "    # Init URL list and start index for a new day\n",
    "    url_list = []\n",
    "    URL_START_IDX = 1\n",
    "    \n",
    "    while True:\n",
    "        # Get URL\n",
    "        url_list_page = url_base.format(QUERY_PARSED, date, date, URL_START_IDX)\n",
    "\n",
    "        # Parse HTML\n",
    "        soup = request_for_soup(url_list_page, headers)\n",
    "        if is_list_page_empty(soup):\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Extend URL list of the current date\n",
    "        url_list.extend([url.get('href') for url in soup.find_all('a', class_='info') if '네이버뉴스' in url])\n",
    "        \n",
    "        # Sleep and move to next page of the URLs\n",
    "        SLEEP_TIME = get_sleep_time_random()\n",
    "        time.sleep(SLEEP_TIME)\n",
    "        URL_START_IDX += 10\n",
    "    \n",
    "    # Export URL list\n",
    "    date_for_dirname = remove_date_sep(date)\n",
    "    FNAME_URL_LIST = '{}.json'.format(date_for_dirname)\n",
    "    FPATH_URL_LIST = os.path.join(FDIR_URL_LIST, FNAME_URL_LIST)\n",
    "    with open(FPATH_URL_LIST, 'w', encoding='utf-8') as f:\n",
    "        json.dump(url_list, f)\n",
    "        \n",
    "    # Extend total URL list\n",
    "    URL_COUNT += len(url_list)\n",
    "    print('URL list (current day): {}'.format(len(url_list)))\n",
    "    print('URL list (total)      : {}'.format(URL_COUNT))\n",
    "    \n",
    "print('=='*30)\n",
    "print('Done: {}'.format(URL_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f6818a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 20230701 / Total Success: 21 / Errors: 0\n",
      "Date: 20230702 / Total Success: 83 / Errors: 0\n",
      "Date: 20230703 / Total Success: 276 / Errors: 0\n",
      "Date: 20230704 / Total Success: 429 / Errors: 0\n",
      "Date: 20230705 / Total Success: 652 / Errors: 0\n",
      "Date: 20230706 / Total Success: 774 / Errors: 4\n",
      "Date: 20230707 / Total Success: 818 / Errors: 7\n",
      "Date: 20230708 / Total Success: 835 / Errors: 13\n",
      "Date: 20230709 / Total Success: 905 / Errors: 14\n",
      "Date: 20230710 / Total Success: 1,051 / Errors: 17\n",
      "Date: 20230711 / Total Success: 1,164 / Errors: 22\n",
      "Date: 20230712 / Total Success: 1,292 / Errors: 23\n",
      "Date: 20230713 / Total Success: 1,508 / Errors: 25\n",
      "Date: 20230714 / Total Success: 1,604 / Errors: 25\n",
      "Date: 20230715 / Total Success: 1,612 / Errors: 25\n",
      "Date: 20230716 / Total Success: 1,662 / Errors: 25\n",
      "Date: 20230717 / Total Success: 1,716 / Errors: 25\n",
      "Date: 20230718 / Total Success: 1,852 / Errors: 25\n",
      "Date: 20230719 / Total Success: 1,949 / Errors: 27\n",
      "Date: 20230720 / Total Success: 2,161 / Errors: 27\n",
      "Date: 20230721 / Total Success: 2,220 / Errors: 27\n",
      "Date: 20230722 / Total Success: 2,246 / Errors: 28\n",
      "Date: 20230723 / Total Success: 2,307 / Errors: 28\n",
      "Date: 20230724 / Total Success: 2,418 / Errors: 28\n",
      "Date: 20230725 / Total Success: 2,556 / Errors: 29\n",
      "Date: 20230726 / Total Success: 2,911 / Errors: 29\n",
      "Date: 20230727 / Total Success: 3,052 / Errors: 29\n",
      "Date: 20230728 / Total Success: 3,146 / Errors: 43\n",
      "Date: 20230729 / Total Success: 3,163 / Errors: 43\n",
      "Date: 20230730 / Total Success: 3,172 / Errors: 43\n",
      "============================================================\n",
      "Done\n",
      "Total   : 30\n",
      "Articles: 3,172\n",
      "Errors  : 43\n",
      "------------------------------------------------------------\n",
      "['https://sports.news.naver.com/news.nhn?oid=481&aid=0000008580', 'https://sports.news.naver.com/news.nhn?oid=076&aid=0004028260', 'https://sports.news.naver.com/news.nhn?oid=654&aid=0000045222', 'https://sports.news.naver.com/news.nhn?oid=119&aid=0002727950', 'https://n.news.naver.com/mnews/article/140/0000050658?sid=106', 'https://sports.news.naver.com/news.nhn?oid=018&aid=0005524732', 'https://sports.news.naver.com/news.nhn?oid=076&aid=0004028888', 'https://sports.news.naver.com/news.nhn?oid=410&aid=0000948400', 'https://sports.news.naver.com/news.nhn?oid=003&aid=0011961463', 'https://sports.news.naver.com/news.nhn?oid=436&aid=0000074824', 'https://sports.news.naver.com/news.nhn?oid=076&aid=0004029391', 'https://sports.news.naver.com/news.nhn?oid=001&aid=0014053489', 'https://sports.news.naver.com/news.nhn?oid=076&aid=0004029244', 'https://n.news.naver.com/mnews/article/076/0004029571?sid=106', 'https://sports.news.naver.com/news.nhn?oid=076&aid=0004030200', 'https://sports.news.naver.com/news.nhn?oid=436&aid=0000074936', 'https://sports.news.naver.com/news.nhn?oid=413&aid=0000162675', 'https://sports.news.naver.com/news.nhn?oid=421&aid=0006921243', 'https://sports.news.naver.com/news.nhn?oid=241&aid=0003287410', 'https://sports.news.naver.com/news.nhn?oid=055&aid=0001072182', 'https://sports.news.naver.com/news.nhn?oid=011&aid=0004212435', 'https://sports.news.naver.com/news.nhn?oid=117&aid=0003749780', 'https://sports.news.naver.com/news.nhn?oid=119&aid=0002729926', 'https://sports.news.naver.com/news.nhn?oid=003&aid=0011970383', 'https://sports.news.naver.com/news.nhn?oid=445&aid=0000126840', 'https://sports.news.naver.com/news.nhn?oid=421&aid=0006938592', 'https://sports.news.naver.com/news.nhn?oid=425&aid=0000141994', 'https://sports.news.naver.com/news.nhn?oid=076&aid=0004034870', 'https://sports.news.naver.com/news.nhn?oid=477&aid=0000442202', 'https://sports.news.naver.com/news.nhn?oid=472&aid=0000027480', 'https://sports.news.naver.com/news.nhn?oid=477&aid=0000442881', 'https://sports.news.naver.com/news.nhn?oid=079&aid=0003796291', 'https://sports.news.naver.com/news.nhn?oid=018&aid=0005540061', 'https://sports.news.naver.com/news.nhn?oid=421&aid=0006959085', 'https://sports.news.naver.com/news.nhn?oid=409&aid=0000020658', 'https://sports.news.naver.com/news.nhn?oid=410&aid=0000951757', 'https://sports.news.naver.com/news.nhn?oid=109&aid=0004896951', 'https://sports.news.naver.com/news.nhn?oid=396&aid=0000650992', 'https://sports.news.naver.com/news.nhn?oid=119&aid=0002735174', 'https://sports.news.naver.com/news.nhn?oid=450&aid=0000094785', 'https://sports.news.naver.com/news.nhn?oid=031&aid=0000762138', 'https://sports.news.naver.com/news.nhn?oid=076&aid=0004037465', 'https://sports.news.naver.com/news.nhn?oid=117&aid=0003755444']\n"
     ]
    }
   ],
   "source": [
    "url_dict = import_url_dict(FDIR_URL_LIST)\n",
    "\n",
    "url_count = len(url_dict.values())\n",
    "article_count_total = 0\n",
    "errors = []\n",
    "\n",
    "for url_date, url_list in url_dict.items():\n",
    "    article_count_date = 0\n",
    "    for url_article in url_list:\n",
    "        # Check existence\n",
    "        article_id = '{:04d}'.format(article_count_date)\n",
    "        FNAME_ARTICLE = '{}/{}.json'.format(remove_date_sep(url_date), article_id)\n",
    "        FPATH_ARTICLE = os.path.join(FDIR_ARTICLE, FNAME_ARTICLE)\n",
    "        \n",
    "        if os.path.isfile(FPATH_ARTICLE):\n",
    "            article_count_total += 1\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = request_for_soup(url_article, headers)\n",
    "\n",
    "        # Parse article\n",
    "        try:\n",
    "            title, date, content = parse_article(soup)\n",
    "        except:\n",
    "            errors.append(url_article)\n",
    "            continue\n",
    "\n",
    "        article = {\n",
    "            'title': title,\n",
    "            'date': date,\n",
    "            'content': content\n",
    "        }\n",
    "\n",
    "        # Save article\n",
    "        make_dir_soft(FPATH_ARTICLE)\n",
    "        with open(FPATH_ARTICLE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(article, f)\n",
    "            article_count_date += 1\n",
    "            article_count_total += 1\n",
    "            \n",
    "        # Sleep and move to next page of the URLs\n",
    "        SLEEP_TIME = get_sleep_time_random()\n",
    "        time.sleep(SLEEP_TIME)\n",
    "\n",
    "    # Print status\n",
    "    print('Date: {} / Total Success: {:,} / Errors: {:,}'.format(url_date, article_count_total, len(errors)))\n",
    "    \n",
    "# Print result\n",
    "print('=='*30)\n",
    "print('Done')\n",
    "print('Total   : {:,}'.format(url_count))\n",
    "print('Articles: {:,}'.format(article_count_total))\n",
    "print('Errors  : {:,}'.format(len(errors)))\n",
    "print('--'*30)\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2a50d",
   "metadata": {},
   "source": [
    "# Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "979d5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315028d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_fpath_article(fdir_article):\n",
    "    for date in os.listdir(fdir_article):\n",
    "        fdir_article_date = os.path.join(fdir_article, date)\n",
    "        flist_article_date = os.listdir(fdir_article_date)\n",
    "        for fname in flist_article_date:\n",
    "            fpath = os.path.join(fdir_article_date, fname)\n",
    "            yield fpath\n",
    "            \n",
    "def load_articles(fdir_article):\n",
    "    articles = []\n",
    "    for fpath in iter_fpath_article(fdir_article):\n",
    "        article_id, _ = '/'.join(fpath.replace('\\\\', '/').split('/')[-2:]).split('.')\n",
    "        with open(fpath, 'r', encoding='utf-8') as f:\n",
    "            article = json.load(f)\n",
    "            article['id'] = article_id\n",
    "            \n",
    "            articles.append(article)\n",
    "            \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613cdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e66488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db541d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd2e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe08c49d",
   "metadata": {},
   "source": [
    "train_test_split\n",
    "\n",
    "- test_size: 전체 데이터에서 test 데이터의 비율\n",
    "- random_state: 데이터를 train과 test로 나누는 방식 --> 동일한 random_state라면 동일하게 구분됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72e450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3a168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3384b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b8fbb8",
   "metadata": {},
   "source": [
    "# Data Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477e977",
   "metadata": {},
   "source": [
    "데이터 라벨링을 해봅시다.\n",
    "- 1: 자동차 판매 관련 기사\n",
    "- 0: 그 외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f7ee5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb72f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14159d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4af6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c3a6044",
   "metadata": {},
   "source": [
    "자동차 판매 관련 기사의 label을 1로 변경한 뒤 \"\\~\\~\\_after_<이름>.xlsx\"의 파일명으로 저장\n",
    "- 김가영: 0~99\n",
    "- 김서연: 100~199\n",
    "- 김현태: 200~299\n",
    "- 민경제: 300~399\n",
    "- 박경훈: 400~499\n",
    "- 박규리: 500~599\n",
    "- 박유민: 600~699\n",
    "- 변성준: 700~799\n",
    "- 서유탁: 800~899\n",
    "- 서준혁: 900~999\n",
    "- 성무선: 1,000~1,099\n",
    "- 성해준: 1,100~1,199\n",
    "- 신서빈: 1,200~1,299\n",
    "- 이석희: 1,300~1,399\n",
    "- 이주연: 1,400~1,499\n",
    "- 한승우: 1,500~1,599\n",
    "- 홍정화: 1,600~1,699\n",
    "\n",
    "---\n",
    "파일명 예시: \"\\~/from20230701to20230730_after_홍길동.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759065b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
